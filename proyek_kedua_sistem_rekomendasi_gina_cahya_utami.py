# -*- coding: utf-8 -*-
"""Proyek Kedua Sistem Rekomendasi - Gina Cahya Utami.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BF6CtxxW72TdT7d3bKfDF_wYH46epn9L

*   Nama : Gina Cahya Utami
*   MSID : M182V4139
*   Username : Gina Cahya
*   Email : ginacu.gc@gmail.com
*   Dataset : https://www.kaggle.com/arashnic/book-recommendation-dataset

# Akses Data ke Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import Library"""

import pandas as pd

"""# Data Loading"""

user = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Proyek Kedua Sistem Rekomendasi/Users.csv')
ratings = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Proyek Kedua Sistem Rekomendasi/Ratings.csv')
books = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Proyek Kedua Sistem Rekomendasi/Books.csv')

"""# Data Understanding"""

print(user.head())

print(ratings.head())

print(books.head())

"""# Univariate Exploratory Data Analysis

Variabel-variabel pada Book Recommendation dataset adalah sebagai berikut:
* user = merupakan data pengguna
* ratings = merupakan informasi seputar rating buku
* book = merupakan informasi mengenai data rincian buku

Variabel book dan ratings akan digunakan pada model rekomendasi. Sedangkan, variabel user hanya untuk melihat profil pengguna.

## User Variable
"""

user.info()

"""Karena terdapat lebih dari 200.000 data, saya hanya akan mengambil 7.000 data pertama dari variabel user untuk digunakan dalam pembuatan model sistem rekomendasi ini"""

user = user[1:7001]
user.info()

"""## Ratings Variable

"""

ratings.info()

"""Karena terdapat lebih dari 1.000.000 data, saya hanya akan mengambil 7.000 data pertama dari variabel ratings untuk digunakan dalam pembuatan model sistem rekomendasi ini"""

ratings = ratings[1:7001]
ratings.info()

ratings.describe()

"""Minimum nilai rating yakni 0 dan maksimumnya yakni 10. Sehingga ratingnya yakni dari skala 0-10.

Melihat berapa pengguna yang memberikan rating, jumlah buku, dan jumlah rating
"""

print('Jumlah user-ID: ', len(ratings['User-ID'].unique()))
print('Jumlah ISBN: ', len(ratings['ISBN'].unique()))
print('Jumlah data rating buku: ', len(ratings['Book-Rating']))

"""## Book Variable"""

books.info()

"""Karena terdapat lebih dari 200.000 data, saya hanya akan mengambil 7.000 data pertama dari variabel books untuk digunakan dalam pembuatan model sistem rekomendasi ini"""

books = books[1:7001]
books.info()

"""Kita akan melihat berapa banyak judul buku yang unik/berbeda satu sama lain dan menampilkan apa saja judul buku tersebut."""

print('Banyak Judul Buku: ', len(books['Book-Title'].unique()))
print('Judul Buku: ', books['Book-Title'].unique())

"""Terdapat 6.750 judul buku yang berbeda seperti terlihat pada output kode.

# Data Preprocessing

Pertama-tama kita hapus dulu ketiga kolom image dalam variabel books dikarenakan kolom tersebut tidak terlalu digunakan
"""

books = books.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1)
books

"""Cek apakah ada nilai yang kosong di dalam data ratings."""

ratings.isnull().sum()

"""Hitung jumlah rating kemudian menggabungkannya berdasarkan ISBN"""

ratings.groupby('ISBN').sum()

"""## Menggabungkan Fitur Ratings dengan Fitur Books

Pertama-tama, kita definisikan variabel all_ratings dengan variabel ratings yang telah kita ketahui sebelumnya.
"""

all_ratings = ratings
all_ratings

"""Gabungkan dataframe all_ratings dengan books dan memasukkannya ke dalam variabel all_books"""

all_books = pd.merge(all_ratings, books[['ISBN','Book-Title','Book-Author']], on='ISBN', how='left')
all_books

"""# Data Preparation

## Mengatasi Missing Value
"""

all_books.isnull().sum()

len(all_books)

"""Membersihkan missing value dengan fungsi dropna()"""

all_books_clean = all_books.dropna()
all_books_clean

all_books_clean.isnull().sum()

"""Urutkan Buku berdasarkan nomor ISBN kemudian memasukkannya ke dalam variabel preparation"""

preparation = all_books_clean.sort_values('ISBN', ascending=True)
preparation

"""## Mengatasi data duplikat pada variabel preparation"""

preparation = preparation.drop_duplicates('ISBN')
preparation

"""## Konversi data series menjadi list

Untuk mengonversi data series menjadi list, kita menggunakan fungsi tolist() dari library numpy.
"""

# Mengonversi data series 'ISBN' menjadi dalam bentuk list
book_id = preparation['ISBN'].tolist()
 
# Mengonversi data series 'Book-Author' menjadi dalam bentuk list
book_author = preparation['Book-Author'].tolist()
 
# Mengonversi data series 'Book-Title' menjadi dalam bentuk list
book_title = preparation['Book-Title'].tolist()
 
print(len(book_id))
print(len(book_author))
print(len(book_title))

"""## Membuat dictionary untuk data book_title, book_author, book_id

Membuat dictionary untuk menentukan pasangan key-value pada data book_id, book_author, dan book_title
"""

book_new = pd.DataFrame({
    'id': book_id,
    'book_author': book_author,
    'book_title': book_title
})
book_new

"""# Model Development dengan Content Based Filtering

### TF-IDF Vectorizer

TF-IDF Vectorizer akan digunakan pada sistem rekomendasi untuk menemukan representasi fitur penting dari setiap kategori buku. Kita akan menggunakan fungsi tfidfvectorizer() dari library sklearn.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data 'book_title'
tf.fit(book_new['book_title']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(book_new['book_title']) 
 
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan judul buku
# Baris diisi dengan author buku
 
pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=book_new.book_author
).sample(1541, axis=1).sample(10, axis=0)

"""### Cosine Similarity"""

# Menghitung cosine similarity pada matrix tf-idf
from sklearn.metrics.pairwise import cosine_similarity
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa author buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=book_new['book_author'], columns=book_new['book_author'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap author buku
cosine_sim_df.sample(7, axis=1).sample(10, axis=0)

"""### Mendapatkan Rekomendasi"""

def book_recommendations(nama_author, similarity_data=cosine_sim_df, items=book_new[['book_author', 'book_title']], n=10):
    
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_author].to_numpy().argpartition(
        range(-1, -n, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(n+2):-1]]
    
    # Drop nama_author agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_author, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(n)

book_new[book_new.book_author.eq('Peter Carey')]

# Mendapatkan rekomendasi buku yang mirip dengan buku dari author Kate White
book_recommendations('Peter Carey')

"""## Evaluasi

### 1. Precision pada Top-N

Precision pada Top-N merupakan proporsi item yang direkomendasikan dalam set    Top-N yang relevan. Berikut rumusnya :
    
* Precision = ((k of recommendation that are relevant) / (k of item we recommend)) . 100 %
* Pada contoh rekomendasi buku dapat kita simpulkan bahwa :
* k of recommendation that are relevant = 8 buku
* k of item we recommend = 10 buku
* Precision = ((8)/(10)) . 100 %
* Jadi presisinya = 80%

### 2. Recall

Recall pada Top-N, merupakan proporsi item relevan yang ditemukan di rekomendasi top-N. Berikut rumusnya :
* Recall = (k of recommendation that are relevant) / (total k of relevant items)
* Pada contoh rekomendasi buku dapat kita simpulkan bahwa :
* a. Pada recall@6
    * k of recommendation that are relevant = 5 buku
    * total k of relevant items = 8 buku
    * Recall @ 6 = (5)/(8)
    * Jadi presisinya = 5/8
        
* b. Pada recall@10
    * k of recommendation that are relevant = 8 buku
    * total k of relevant items = 8 buku
    * Recall @ 10 = (8)/(8)
    * Jadi presisinya = 8/8

# Model Development dengan Collaborative Filtering

### Import Library
"""

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""## Data Understanding

### Data Loading
"""

# Membaca dataset
df = ratings
df

"""## Data Preparation

Melakukan persiapan data untuk menyandikan (encode) fitur 'User-ID' dan 'ISBN' ke dalam indeks integer.
"""

# Mengubah User-ID menjadi list tanpa nilai yang sama
user_ids = df['User-ID'].unique().tolist()
print('list User-ID: ', user_ids)
 
# Melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User-ID : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User-ID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = df['ISBN'].unique().tolist()
 
# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
 
# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}
 
# Selanjutnya, petakan User-ID dan ISBN ke dataframe yang berkaitan.
 
# Mapping User-ID ke dataframe user
df['user'] = df['User-ID'].map(user_to_user_encoded)
 
# Mapping ISBN ke dataframe buku
df['book'] = df['ISBN'].map(book_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah buku
num_book = len(book_encoded_to_book)
print(num_book)
 
# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['Book-Rating'])
 
# Nilai maksimal rating
max_rating = max(df['Book-Rating'])
 
print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""### Membagi Data untuk Training dan Validasi"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""## Proses Training"""

class RecommenderNet(tf.keras.Model):
  
  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book, 50)
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

book_df = book_new
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Proyek Kedua Sistem Rekomendasi/Ratings.csv')
df = data[1:7001]

# Mengambil sample user
user_id = df['User-ID'].sample(100).iloc[30]
book_visited_by_user = df[df['User-ID'] == user_id]
 
# Operator bitwise (~)
book_not_visited = book_df[~book_df['id'].isin(book_visited_by_user.ISBN.values)]['id'] 
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_visited = [[book_to_book_encoded.get(x)] for x in book_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited)
)

rating = model.predict(user_book_array).flatten()
 
top_rating_indices = rating.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_visited[x][0]) for x in top_rating_indices
]
 
print('Menampilkan Rekomendasi  Book_Author untuk Pengguna dengan Total : {} Pengguna'.format(user_id))
print('===' * 11)
top_book_user = (
    book_visited_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = book_df[book_df['id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.book_author, ':', row.book_title)
 
print('---' * 11)
print('Top 10 Book_Author Recommendation')
print('---' * 11)
 
recommended_book = book_df[book_df['id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.book_author, ':', row.book_title)

"""## Evaluasi

### 1. Metrik Root Mean Squared Error (RMSE)
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## 2. Mean Squared Error (MSE)"""

from sklearn.metrics import mean_squared_error
print("MSE dari pada data train = ", mean_squared_error(y_true=y_train, y_pred=model.predict(x_train))/1e3)
print("MSE dari pada data validation = ", mean_squared_error(y_true=y_val, y_pred=model.predict(x_val))/1e3)

